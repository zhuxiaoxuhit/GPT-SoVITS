data:
  max_eval_sample: 8
  max_sec: 54
  num_workers: 4
  pad_val: 1024
inference:
  top_k: 5
model:
  EOS: 1024
  dropout: 0
  embedding_dim: 512
  head: 16
  hidden_dim: 512
  linear_units: 2048
  n_layer: 24
  phoneme_vocab_size: 512
  random_bert: 0
  vocab_size: 1025
optimizer:
  decay_steps: 40000
  lr: 0.01
  lr_end: 0.0001
  lr_init: 1.0e-05
  warmup_steps: 2000
output_dir: logs/xxx/logs_s1
pretrained_s1: GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt
train:
  batch_size: 4
  epochs: 2
  exp_name: xxx
  gradient_clip: 1.0
  half_weights_save_dir: GPT_weights
  if_dpo: false
  if_save_every_weights: true
  if_save_latest: true
  precision: '32'
  save_every_n_epoch: 5
  seed: 1234
train_phoneme_path: logs/xxx/2-name2text.txt
train_semantic_path: logs/xxx/6-name2semantic.tsv
